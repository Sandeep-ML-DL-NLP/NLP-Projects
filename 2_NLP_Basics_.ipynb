{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc/rSLASNIWeqI72zppeTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandeep-ML-DL-NLP/NLP-Projects/blob/main/2_NLP_Basics_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WHrFLVHy1UH",
        "outputId": "d05b2dfb-a168-4816-ffc8-bd6dfa39958b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n",
            "[[0 1 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 1]\n",
            " [1 0 1 1 1 0 1 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# building vectors and matrices based on text data.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "X = (\"Computers can analyze text\",\n",
        " \"They do it using vectors and matrices\",\n",
        " \"Computers can process massive amounts of text data\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "print(vectorizer.vocabulary_) \n",
        "print(X_vec.todense())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re \n",
        "from nltk.tokenize import TreebankWordDetokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep-9yvcEzhHC",
        "outputId": "82f44a15-acda-4ff5-c855-edc065bc74fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "\"Natural Language Processing making computers comprehend language \\\n",
        "data\",\n",
        "\"The field of Natural Language Processing is evolving everyday\"]\n",
        "corpus = pd.Series(sentences)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJCuFzKN1sa8",
        "outputId": "4d7d989a-d8f2-4a8b-d5fa-3741b5ad7485"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "    \n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "    \n",
        "    Output : Returns the cleaned text corpus\n",
        "    \n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "t2U-b1FP2JlC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = stopwords\n",
        "\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "6r1PiG499GKl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "6PRVoe7wBdXA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "kBc5CovFBgDN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "    \n",
        "    Input : \n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "    \n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "    \n",
        "    Output : Returns the processed text corpus\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "    \n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "    \n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "        \n",
        "        \n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "    \n",
        "    corpus = [' '.join(x) for x in corpus]        \n",
        "\n",
        "    return corpus\n",
        "common_dot_words = ['U.S.A', 'Mr.', 'Mrs.', 'D.C.']"
      ],
      "metadata": {
        "id": "AAGL0gOHBh_3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus = preprocess(corpus, \\\n",
        " keep_list = common_dot_words, stemming = False, \\\n",
        " stem_type = None, lemmatization = True, \\\n",
        " remove_stopwords = True)\n",
        "preprocessed_corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZHn6fDOBkmw",
        "outputId": "2bea8bb1-afa9-40e5-e6e1-449ca78caf50"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-a7d5d0d93ac1>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-41-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-41-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-41-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build your vocabulary\n",
        "set_of_words = set()\n",
        "for sentences in preprocessed_corpus:\n",
        "  for word in sentences.split():\n",
        "    set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGgTnudnBpPZ",
        "outputId": "fc4eb973-8f1e-4813-9607-ebff7278a7f4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['make',\n",
              " 'natural',\n",
              " 'field',\n",
              " 'evolve',\n",
              " 'data',\n",
              " 'process',\n",
              " 'everyday',\n",
              " 'language',\n",
              " 'read',\n",
              " 'comprehend',\n",
              " 'computers']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i,token in enumerate(vocab):\n",
        "  position[token] = i\n",
        "\n",
        "position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z27OyjaDXTX",
        "outputId": "eea840e3-9bab-4ab2-9454-3c796e9ecd5e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'make': 0,\n",
              " 'natural': 1,\n",
              " 'field': 2,\n",
              " 'evolve': 3,\n",
              " 'data': 4,\n",
              " 'process': 5,\n",
              " 'everyday': 6,\n",
              " 'language': 7,\n",
              " 'read': 8,\n",
              " 'comprehend': 9,\n",
              " 'computers': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = np.zeros((len(preprocessed_corpus),len(vocab)))\n",
        "bow_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd-01lWIStZR",
        "outputId": "1ee46264-9a9f-4402-b552-e56daa0422d9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,preprocessed_sentence in enumerate(preprocessed_corpus):\n",
        "  for token in preprocessed_sentence.split():\n",
        "     bow_matrix[i][position[token]] = bow_matrix[i][position[token]]+1\n",
        "bow_matrix "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQK7CAa-TLqk",
        "outputId": "907d70f5-39f4-46b8-9b6e-7053f82ac042"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
              "       [1., 1., 0., 0., 1., 1., 0., 2., 0., 1., 1.],\n",
              "       [0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hey! Do I need to code all this up? Doesn't any Python library provide all this as an inbuilt functionality ?\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "o0MsnjWCUH0e"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X48Wx2jfVMvt",
        "outputId": "03fefdae-c2dd-4909-af29-37144cc68112"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
              "       [1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer with ngrams\n",
        "vectorizer_ngram_range = CountVectorizer(analyzer='word',\n",
        "ngram_range=(1,3))\n",
        "bow_matrix_ngram = \\\n",
        "vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_ngram_range.vocabulary_)\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j6v4gPqVV02",
        "outputId": "4bddcc86-a26c-444b-cb7d-82d8ccb4f48e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'read': 29, 'natural': 21, 'language': 13, 'process': 24, 'read natural': 30, 'natural language': 22, 'language process': 15, 'read natural language': 31, 'natural language process': 23, 'make': 18, 'computers': 3, 'comprehend': 0, 'data': 6, 'process make': 27, 'make computers': 19, 'computers comprehend': 4, 'comprehend language': 1, 'language data': 14, 'language process make': 17, 'process make computers': 28, 'make computers comprehend': 20, 'computers comprehend language': 5, 'comprehend language data': 2, 'field': 10, 'evolve': 8, 'everyday': 7, 'field natural': 11, 'process evolve': 25, 'evolve everyday': 9, 'field natural language': 12, 'language process evolve': 16, 'process evolve everyday': 26}\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer(norm=\"l1\")\n",
        "tf_matrix = tf.fit_transform(preprocessed_corpus)\n",
        "tf_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UizjHMB8WZ9Y",
        "outputId": "d007fe55-8fad-4720-e5f9-9d86d7f2d9eb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.21307663, 0.        , 0.21307663, 0.21307663,\n",
              "        0.3607701 ],\n",
              "       [0.1571718 , 0.1571718 , 0.1571718 , 0.        , 0.        ,\n",
              "        0.        , 0.1856564 , 0.1571718 , 0.0928282 , 0.0928282 ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.2095624 , 0.2095624 ,\n",
              "        0.2095624 , 0.12377093, 0.        , 0.12377093, 0.12377093,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GotJEScGeZ1h",
        "outputId": "066ba90a-6764-4f10-a4a7-5e55d7ede5ab"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot vectorization\n",
        "from nltk.corpus import stopwords\n",
        "stem = PorterStemmer()\n",
        "stem2 = SnowballStemmer(language='english')\n",
        "sentence = [\"We are reading about Natural Language Processing \\\n",
        "Here\"]\n",
        "corpus = pd.Series(sentence)\n",
        "for rows in corpus:\n",
        "  tokens = [token.lower() for token in rows.split() if token.lower() not in stopwords.words('english')]\n",
        "  preprocess_word = [stem.stem(token) for token in tokens]\n",
        "  preprocessed_word = [stem2.stem(token) for token in tokens]\n",
        "  print(preprocess_word)\n",
        "  print(preprocessed_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "700Fc3bhelAk",
        "outputId": "f5f2eacb-0305-4575-d46c-bf757327081c"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['read', 'natur', 'languag', 'process']\n",
            "['read', 'natur', 'languag', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position2 = {}\n",
        "for i,token in enumerate(preprocess_word):\n",
        "  position2[token] = i\n",
        "position2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC2NJKQ-iJ7s",
        "outputId": "ee3fb695-2eec-4b90-9716-9eb8966de0d2"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'read': 0, 'natur': 1, 'languag': 2, 'process': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix = np.zeros((4,4))\n",
        "for i,token in enumerate(preprocess_word):\n",
        "  one_hot_matrix[i][position2[token]] = 1\n",
        "one_hot_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofyVj_iGiNha",
        "outputId": "483c8518-fbc8-49f0-acd8-c09a75cc2774"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tclOAvFm5xV"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g07-AVeXqFge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}